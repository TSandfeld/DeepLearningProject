{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project\n",
    "By Thomas Sandfeld Nielsen and Lennart J. Pedersen\n",
    "\n",
    "Choose: Cell > Run All.\n",
    "\n",
    "### WARNING: \n",
    "The model consumes upward of 10 GBs of RAM. To run locally, switch the '#' mark before the call to create_network()\n",
    "just before the section 'Training, loss and performance'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import utils\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "num_classes = 12\n",
    "height, width, nchannels = 224, 224, 3\n",
    "padding = 'same'\n",
    "\n",
    "x_pl = tf.placeholder(tf.float32, [None, height, width, nchannels], name='xPlaceholder')\n",
    "y_pl = tf.placeholder(tf.float32, [None, height, width, num_classes], name='yPlaceholder')\n",
    "\n",
    "DENSE_BLOCK_I = 1\n",
    "TD_I = 1\n",
    "TU_I = 1\n",
    "CONCAT_I = 1\n",
    "\n",
    "OUTPUT_SIZE = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2 = slim.l2_regularizer(0.0001)\n",
    "regularizers = {\"beta\" : slim.l2_regularizer(0.0001), \"gamma\": slim.l2_regularizer(0.0001)}\n",
    "\n",
    "def concatenate(x, net):\n",
    "    global CONCAT_I\n",
    "    with tf.variable_scope('concat%d' % CONCAT_I, reuse=True):\n",
    "        CONCAT_I += 1\n",
    "        return tf.concat([x, net], axis=-1)\n",
    "\n",
    "def concatenateN(xs, net):\n",
    "    global CONCAT_I\n",
    "    with tf.variable_scope('concat%d' % CONCAT_I, reuse=True):\n",
    "        to_cat = []\n",
    "        for x in xs:\n",
    "            to_cat.append(x)\n",
    "            \n",
    "        to_cat.append(net)\n",
    "        CONCAT_I += 1\n",
    "        return tf.concat(to_cat, axis=-1)\n",
    "\n",
    "def denseBlock(n, net, num_features, is_training):\n",
    "    global DENSE_BLOCK_I\n",
    "    with tf.variable_scope( 'Denseblock_%d' % (DENSE_BLOCK_I)):\n",
    "        last_x = net\n",
    "        dense_layer_outputs = []\n",
    "    \n",
    "        # For N-1 do left side of dense block diagram (it makes sense)\n",
    "        for i in range(n-1):\n",
    "            with tf.variable_scope( 'db_layer%d_%d' % (DENSE_BLOCK_I,i+1)):\n",
    "                x = slim.batch_norm(last_x, activation_fn=tf.nn.relu, param_regularizers=regularizers)\n",
    "                x = slim.conv2d(inputs=x, kernel_size=[3,3], num_outputs=num_features, padding='same', weights_initializer=tf.contrib.keras.initializers.he_uniform(), weights_regularizer=l2, activation_fn=None)\n",
    "                x = slim.dropout(inputs=x, keep_prob=0.8, is_training=is_training)\n",
    "                dense_layer_outputs.append(x)\n",
    "                last_x = concatenate(last_x, x)\n",
    "    \n",
    "        final_x = 0\n",
    "        # For last N gather all x outputs from layers in dense block and concatenate with the last output\n",
    "        with tf.variable_scope( 'db_layer%d_%d' % (DENSE_BLOCK_I,n)):\n",
    "            x = slim.batch_norm(last_x, activation_fn=tf.nn.relu, param_regularizers=regularizers)\n",
    "            x = slim.conv2d(inputs=x, kernel_size=[3,3], num_outputs=num_features, padding='same', weights_initializer=tf.contrib.keras.initializers.he_uniform(), weights_regularizer=l2, activation_fn=None)\n",
    "            x = slim.dropout(inputs=x, keep_prob=0.8, is_training=is_training)\n",
    "\n",
    "        final_x = x\n",
    "        final_x = concatenateN(dense_layer_outputs, final_x)\n",
    "\n",
    "    DENSE_BLOCK_I += 1\n",
    "\n",
    "    return final_x\n",
    "\n",
    "def transition_down(net, num_features, is_training):\n",
    "    global TD_I\n",
    "    with tf.variable_scope( 'TD%d' % TD_I):\n",
    "        x = slim.batch_norm(net, activation_fn=tf.nn.relu, param_regularizers=regularizers)\n",
    "        x = slim.conv2d(inputs=x, kernel_size=[1,1], num_outputs=num_features, padding='same', weights_initializer=tf.contrib.keras.initializers.he_uniform(), weights_regularizer=l2, activation_fn=None)\n",
    "        x = slim.dropout(inputs=x, keep_prob=0.8, is_training=is_training)\n",
    "        x = slim.max_pool2d(inputs=x, kernel_size=[2,2])\n",
    "    \n",
    "    TD_I += 1\n",
    "    return x\n",
    "\n",
    "def transition_up(net, num_features):\n",
    "    global TU_I\n",
    "    with tf.variable_scope( 'TU%d' % TU_I):\n",
    "        # Normal transposed convolution\n",
    "        x = slim.conv2d_transpose(inputs=net, kernel_size=[3,3], stride=2, num_outputs=num_features, padding='same', weights_initializer=tf.contrib.keras.initializers.he_uniform(), weights_regularizer=l2, activation_fn=None)\n",
    "        \n",
    "        # Nearest neighbor interpolation\n",
    "        #newHeigtWidth = int(net.shape[1]) * 2\n",
    "        #x = tf.image.resize_nearest_neighbor(images = net, size =[newHeigtWidth, newHeigtWidth])\n",
    "    \n",
    "    TU_I += 1    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_network(block_layer_sizes, is_training, reuse=None):\n",
    "    indexes = [i for i in range(len(block_layer_sizes))]\n",
    "    block_size = block_layer_sizes[0]\n",
    "    bottleneck_layer_size = 4\n",
    "    skip_dict = {}\n",
    "    \n",
    "    input_data = x_pl\n",
    "    conv_out = slim.conv2d(input_data, OUTPUT_SIZE*block_size, kernel_size=[3,3], activation_fn=None)\n",
    "    \n",
    "    old_num_features = OUTPUT_SIZE*block_size\n",
    "    #Downsampling\n",
    "    new_input = conv_out\n",
    "    for nb in indexes:\n",
    "        n = block_layer_sizes[nb]\n",
    "        feats = n*OUTPUT_SIZE\n",
    "        new_features = feats + old_num_features\n",
    "        \n",
    "        net = denseBlock(n, new_input, OUTPUT_SIZE, is_training)\n",
    "        concat = concatenate(new_input, net)\n",
    "    \n",
    "        skip_dict[nb] = concat # Store the skip connection after concat\n",
    "    \n",
    "        new_input = transition_down(concat, new_features, is_training)\n",
    "        print(new_input)\n",
    "        old_num_features = new_features\n",
    "    \n",
    "    # BOTTLENECK\n",
    "    new_features = block_size*OUTPUT_SIZE + old_num_features\n",
    "    bottleneck_out = denseBlock(bottleneck_layer_size, new_input, OUTPUT_SIZE, is_training)\n",
    "    old_num_features = new_features\n",
    "    \n",
    "    # Upsampling\n",
    "    print('upsampling')\n",
    "    new_input = bottleneck_out\n",
    "    for nb in reversed(indexes):\n",
    "        n = block_layer_sizes[nb]\n",
    "        \n",
    "        feats = n*OUTPUT_SIZE\n",
    "        new_features = old_num_features - feats\n",
    "        old_num_features = new_features\n",
    "        new_features = old_num_features + feats*2\n",
    "        \n",
    "        tu_out = transition_up(new_input, new_features)\n",
    "    \n",
    "        skip_connection = skip_dict[nb] # Read skip connection\n",
    "    \n",
    "        concat = concatenate(tu_out, skip_connection)\n",
    "        new_input = denseBlock(n, concat, OUTPUT_SIZE, is_training)\n",
    "        print(new_input)\n",
    "    \n",
    "    final_input = concatenate(concat, new_input)\n",
    "    conv_finish = slim.conv2d(final_input, OUTPUT_SIZE, kernel_size=[1,1], activation_fn=None, weights_initializer=tf.contrib.keras.initializers.he_uniform(), weights_regularizer=l2)\n",
    "    \n",
    "    return conv_finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block_layer_sizes = [4,4,4,4,4]\n",
    "#block_layer_sizes = [2,2]\n",
    "y = create_network(block_layer_sizes=block_layer_sizes, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, training and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('loss, training, performance')\n",
    "with tf.variable_scope('loss'):\n",
    "    masked_weights = 1 - y_pl[:,:,:,-1]\n",
    "    cross_entropy = tf.losses.softmax_cross_entropy(logits=y, onehot_labels=y_pl, weights=masked_weights)\n",
    "\n",
    "    \n",
    "with tf.variable_scope('training'):\n",
    "    # defining our optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "    # applying the gradients\n",
    "    train_op = optimizer.minimize(cross_entropy)\n",
    "\n",
    "    \n",
    "with tf.variable_scope('performance'):\n",
    "    masked_labels = y_pl[:,:,:,:11]#1 - tf.unstack(y_pl, axis=-1)[-1]\n",
    "    masked_preds = y[:,:,:,:11]#1 - tf.unstack(y, axis=-1)[-1]\n",
    "    #print(masked_preds.shape)\n",
    "    #print(masked_weights.shape)\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    #correct_prediction = tf.equal(tf.argmax(y, axis=-1), tf.argmax(y_pl, axis=-1))\n",
    "    correct_prediction = tf.equal(tf.argmax(masked_preds, axis=-1), tf.argmax(masked_labels, axis=-1))\n",
    "\n",
    "    # averaging the one-hot encoded vector\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print('Model consits of ', utils.num_params(), 'trainable parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "\n",
    "# For the CamVid dataset\n",
    "train_path = './CamVid/train/'\n",
    "train_labels_path = './CamVid/trainannot/'\n",
    "test_path = './CamVid/test/'\n",
    "test_labels_path = './CamVid/testannot/'\n",
    "val_path = './CamVid/val/'\n",
    "val_labels_path = './CamVid/valannot/'\n",
    "\n",
    "train_imgs = [train_path + f for f in listdir(train_path)]\n",
    "train_labels = [train_labels_path + f for f in listdir(train_labels_path)]\n",
    "\n",
    "test_imgs = [test_path + f for f in listdir(test_path)]\n",
    "test_labels = [test_labels_path + f for f in listdir(test_labels_path)]\n",
    "\n",
    "val_imgs = [val_path + f for f in listdir(val_path)]\n",
    "val_labels = [val_labels_path + f for f in listdir(val_labels_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_labels(labels):\n",
    "    new_arr = []\n",
    "    for label in labels:\n",
    "        label_arr = []\n",
    "        for pixel_values in label:\n",
    "            new_pixels = []\n",
    "            for pixel in pixel_values:\n",
    "                new_vals = [0]*12\n",
    "                new_vals[pixel] = 1\n",
    "                new_pixels.append(np.array(new_vals))\n",
    "            label_arr.append(np.array(new_pixels))\n",
    "        new_arr.append(np.array(label_arr))\n",
    "        \n",
    "    return np.array(new_arr)\n",
    "\n",
    "from scipy import misc\n",
    "import random\n",
    "\n",
    "def normalized(rgb):\n",
    "    #return rgb/255.0\n",
    "    norm=np.zeros((rgb.shape[0], rgb.shape[1], 3),np.float32)\n",
    "\n",
    "    b=rgb[:,:,0]\n",
    "    g=rgb[:,:,1]\n",
    "    r=rgb[:,:,2]\n",
    "\n",
    "    norm[:,:,0] = b/255\n",
    "    norm[:,:,1] = g/255\n",
    "    norm[:,:,2] = r/255\n",
    "    \n",
    "    return norm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random as rand\n",
    "def next_batch(num, data, labels, idxs, count):\n",
    "    img_size = (224,224)\n",
    "    \n",
    "    data_shuffle = np.array([misc.imresize(misc.imread(data[i]), img_size) for i in idxs])\n",
    "    \n",
    "    new_data_shuffle = []\n",
    "    for d in data_shuffle:\n",
    "        new_data_shuffle.append(normalized(d))\n",
    "    \n",
    "    data_shuffle = np.array(new_data_shuffle)\n",
    "    \n",
    "    labels_shuffle = np.array([misc.imresize(misc.imread(labels[i]), img_size ) for i in idxs])\n",
    "    labels = one_hot_labels(labels_shuffle)\n",
    "\n",
    "    # Randomly flip images to left and right\n",
    "    new_data = []\n",
    "    new_labels = []\n",
    "    for j in range(len(data_shuffle)):\n",
    "        data = data_shuffle[j]\n",
    "        label = labels[j]\n",
    "        \n",
    "        chance = int(rand.random()*10)\n",
    "        if chance < 6:\n",
    "            if chance < 3:\n",
    "                #print('left right')\n",
    "                new_data.append(np.fliplr(data))\n",
    "                new_labels.append(np.fliplr(label))\n",
    "            else:\n",
    "                #print('rot90')\n",
    "                new_data.append(np.rot90(data))\n",
    "                new_labels.append(np.rot90(label))\n",
    "        else:\n",
    "            new_data.append(data)\n",
    "            new_labels.append(label)\n",
    "    \n",
    "    data_shuffle = np.array(new_data)\n",
    "    labels = np.array(new_labels)\n",
    "    \n",
    "    #If there is not enough data left to get full batch size:\n",
    "    if len(data_shuffle[count * num: ]) < num:\n",
    "        data_shuffle = data_shuffle[count * num:]\n",
    "        labels = labels[count * num:]\n",
    "    else:\n",
    "        #Get the batch of the remaining data:\n",
    "        data_shuffle = data_shuffle[count * num : (count + 1) * num]\n",
    "        labels = labels[count * num : (count + 1) * num]\n",
    "    \n",
    "    return data_shuffle, labels\n",
    "\n",
    "\n",
    "\n",
    "def deencode_one_hot(image):\n",
    "    colors = [(128, 128, 128), (128, 0, 0), (192, 192, 128), (128, 64, 128), (0, 0, 192),\n",
    "         (128, 128, 0), (192, 128, 128), (64, 64, 128), (64, 0, 128), (64, 64, 0),\n",
    "         (0, 128, 192), (0, 0, 0)]\n",
    "    \n",
    "    res = []\n",
    "    for img in image:\n",
    "        row = []\n",
    "        for one_hot in img:\n",
    "            index = 0\n",
    "            for p in one_hot:\n",
    "                if p == 1:\n",
    "                    try:\n",
    "                        row.append(colors[index])\n",
    "                    except:\n",
    "                        #print(index, len(colors))\n",
    "                        continue\n",
    "                index += 1\n",
    "        res.append(np.array(row,dtype=np.float32))\n",
    "    return np.array(res, dtype=np.float32)\n",
    "\n",
    "def deencode_pred(pred):\n",
    "    colors = [(128, 128, 128), (128, 0, 0), (192, 192, 128), (128, 64, 128), (0, 0, 192),\n",
    "         (128, 128, 0), (192, 128, 128), (64, 64, 128), (64, 0, 128), (64, 64, 0),\n",
    "         (0, 128, 192), (0, 0, 0)]\n",
    "    \n",
    "    res = []\n",
    "    for img in pred:\n",
    "        row = []\n",
    "        for p in img:\n",
    "            row.append(colors[p])\n",
    "        res.append(np.array(row, dtype=np.float32))        \n",
    "                \n",
    "    return np.array(res, dtype=np.float32)\n",
    "\n",
    "\n",
    "def get_iou(preds, labels, num_classes, session):\n",
    "    colors = [(128, 128, 128), (128, 0, 0), (192, 192, 128), (128, 64, 128), (0, 0, 192),\n",
    "         (128, 128, 0), (192, 128, 128), (64, 64, 128), (64, 0, 128), (64, 64, 0),\n",
    "         (0, 128, 192)]\n",
    "    names = ['sky', 'building', 'column_pole', 'road', 'sidewalk', 'tree', \n",
    "                'sign', 'fence', 'car', 'pedestrian', 'bicyclist'] \n",
    "    \n",
    "    batch_size = preds.shape[0]\n",
    "    mean_iou = 0.0\n",
    "    iou_per_class = {}\n",
    "    for i in range(batch_size):\n",
    "        prediction = preds[i]\n",
    "        label = labels[i]\n",
    "        \n",
    "        # Convert labels to same format as predictions\n",
    "        new_labels = []\n",
    "        for dimension in label:\n",
    "            arr = []\n",
    "            for pixels in dimension:\n",
    "                arr.append(np.where(pixels==1)[0][0])\n",
    "            new_labels.append(arr)\n",
    "                \n",
    "        label = new_labels\n",
    "        \n",
    "        # Flatten\n",
    "        o = tf.reshape(prediction,[-1])\n",
    "        y = tf.reshape(label,[-1])\n",
    "        \n",
    "        size = y.shape[0]\n",
    "        \n",
    "        o = o.eval(session=session)\n",
    "        y = y.eval(session=session)\n",
    "        \n",
    "        classes_in_label = np.unique(y)\n",
    "        if 11 in classes_in_label:\n",
    "            index = np.where(classes_in_label==11)\n",
    "            classes_in_label = np.delete(classes_in_label, [index])\n",
    "        \n",
    "        for c in classes_in_label:\n",
    "            union = []\n",
    "            l_or = []\n",
    "            \n",
    "            color = colors[c]\n",
    "            class_name = names[c]\n",
    "            for p in range(size):\n",
    "                try:\n",
    "                    label_color = colors[y[p]]\n",
    "                    pred_color = colors[o[p]]\n",
    "                except:\n",
    "                    # fails on void pixels which should not be count in IoU\n",
    "                    continue\n",
    "                union.append(int(pred_color==color and label_color==color))\n",
    "                l_or.append(int(pred_color==color or label_color==color))\n",
    "        \n",
    "            sum_union = sum(union)\n",
    "            sum_l_or = sum(l_or)\n",
    "            if sum_l_or < 1:\n",
    "                sum_l_or = 1\n",
    "                \n",
    "            iou = sum_union/sum_l_or\n",
    "                \n",
    "            try:\n",
    "                iou_per_class[class_name] += iou\n",
    "            except:\n",
    "                iou_per_class[class_name] = iou\n",
    "            \n",
    "            mean_iou += iou\n",
    "    \n",
    "    all_classes_mean_iou = 0.0\n",
    "    for k,v in iou_per_class.items():\n",
    "        print(k, \"IOU :\", v/batch_size)\n",
    "        all_classes_mean_iou += v/batch_size\n",
    "    \n",
    "    return all_classes_mean_iou/num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model \n",
    "Train the model and get a test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "max_epochs = 10\n",
    "\n",
    "\n",
    "valid_loss, valid_accuracy = [], []\n",
    "train_loss, train_accuracy = [], []\n",
    "test_loss, test_accuracy = [], []\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "TRAIN_DATASIZE = len(train_imgs)\n",
    "TEST_DATASIZE = len(test_imgs)\n",
    "train_runs = TRAIN_DATASIZE / batch_size\n",
    "test_runs = TEST_DATASIZE / batch_size\n",
    "\n",
    "train_runs = int(train_runs) + 1\n",
    "test_runs = int(test_runs)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    completed = 0\n",
    "    \n",
    "    try:\n",
    "        while completed < max_epochs:\n",
    "            print('training started')\n",
    "            #Shuffle data before each epoch\n",
    "            idxs = np.random.permutation(TRAIN_DATASIZE)\n",
    "            idxs_val = np.random.permutation(len(val_imgs))\n",
    "            \n",
    "            for i in range(train_runs):       \n",
    "                _train_loss, _train_accuracy = [], []\n",
    "                \n",
    "                ## Run train op\n",
    "                x_batch, y_batch =  next_batch(batch_size, train_imgs, train_labels, idxs, i)\n",
    "                fetches_train = [train_op, cross_entropy, accuracy]\n",
    "                feed_dict_train = {x_pl: x_batch, y_pl: y_batch}\n",
    "                _, _loss, _acc = sess.run(fetches_train, feed_dict_train)\n",
    "                \n",
    "                _train_loss.append(_loss)\n",
    "                _train_accuracy.append(_acc)\n",
    "                \n",
    "                    \n",
    "                if i % 4 == 0:                \n",
    "                    train_loss.append(np.mean(_train_loss))\n",
    "                    train_accuracy.append(np.mean(_train_accuracy))\n",
    "                    \n",
    "                    fetches_valid = [cross_entropy, accuracy]\n",
    "                    val_count = int(i/4)\n",
    "                    x_val, y_val = next_batch(batch_size, val_imgs, val_labels, idxs_val, val_count)\n",
    "                    \n",
    "                    feed_dict_valid = {x_pl: x_val, y_pl: y_val}\n",
    "                    _loss, _acc = sess.run(fetches_valid, feed_dict_valid)\n",
    "                    \n",
    "                    valid_loss.append(_loss)\n",
    "                    valid_accuracy.append(_acc)\n",
    "                    print(\"Epoch {} : Train Loss {:6.3f}, Train acc {:6.3f}, Valid loss {:6.3f}, Valid acc: {:6.3f}\".format(\n",
    "                            completed, train_loss[-1], train_accuracy[-1], valid_loss[-1], valid_accuracy[-1]))\n",
    "                \n",
    "                print(i+1, 'of', train_runs, 'done')\n",
    "            \n",
    "            completed += 1\n",
    "        \n",
    "        y = create_network(block_layer_sizes=block_layer_sizes, is_training=False, reuse=True)\n",
    "        \n",
    "        idxs_test = np.random.permutation(TEST_DATASIZE)\n",
    "        \n",
    "        x_batch, y_batch =  next_batch(batch_size, test_imgs, test_labels, idxs_test, 1)\n",
    "        feed_dict_test = {x_pl: x_batch, y_pl: y_batch}\n",
    "        \n",
    "        fetches_valid = [cross_entropy, accuracy]\n",
    "        _loss, _acc = sess.run(fetches_valid, feed_dict_test)\n",
    "        test_loss.append(_loss)\n",
    "        test_accuracy.append(_acc)\n",
    "        print('Test Loss {:6.3f}, Test acc {:6.3f}'.format(np.mean(test_loss), np.mean(test_accuracy)))\n",
    "        \n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        # Get image\n",
    "        idxs_out = np.random.permutation(len(test_imgs))\n",
    "        \n",
    "        feed_dict = {x_pl: x_batch}\n",
    "        classification = sess.run(tf.argmax(y,axis=-1), feed_dict)\n",
    "        \n",
    "        y_img = deencode_one_hot(y_batch[0])\n",
    "        c_img = deencode_pred(classification[0])\n",
    "        \n",
    "        print(\"IOU = \", get_iou(classification, y_batch, 11, session=sess))\n",
    "        \n",
    "        import scipy\n",
    "        scipy.misc.toimage(x_batch[0],cmin=0.0, cmax=1.0).save('x_img.jpg')\n",
    "        scipy.misc.toimage(y_img,cmin=0.0, cmax=255.0).save('y_img.jpg')\n",
    "        scipy.misc.toimage(c_img,cmin=0.0, cmax=255.0).save('c_img.jpg')\n",
    "        \n",
    "    except IOError as e:\n",
    "        print(e.errno, e.strerror)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python_3.5]",
   "language": "python",
   "name": "conda-env-Python_3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
